{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Optimization\n",
    "\n",
    "Author: Antonio Miranda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all required packages\n",
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import tree\n",
    "from skopt import BayesSearchCV\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the dataset and split the predictors and the response variable in train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pandas.read_csv(\"kaggleCompetition.csv\")\n",
    "data = data.values\n",
    "\n",
    "# Dataset decomposition\n",
    "X = data[0:1460,:-1]\n",
    "y = data[0:1460,-1]\n",
    "X_comp = data[1460:,:-1]\n",
    "y_comp = data[1460:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1. KNN. Grid-search with 3-fold crossvalidation for K hyper-parameter tuning. Model evaluation with 4-fold crossvalidation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility purposes we set a seed.\n",
    "\n",
    "As a design choice we choose to do the Grid Search between 1 and 50 neighbors, in increments of 2 (to have a smaller search space and to select an odd number of neighbors, to avoid ties). According to some documentattion, the rule of thumb is to have a number of neigbors near the square root of the sample size (<40). So we explore the neighbor-space around that number, despite according to our personal experience, much smaller values are usually selected.\n",
    "\n",
    "Since the sample size is not too large and we only have to optimize one parameter, we use use one core. In Windows systems to use more cores we would need to encapsulate the code inside \"if __name__ == '__main__':\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:24.963467\n",
      "[0.04392385 0.05844956 0.06067776 0.04528427]\n",
      "0.05208385962541403 +- 0.007536535431943325\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------- Grid Search ----------------------------------\n",
    "\n",
    "# Hyper-parameter tuning --> Gridsearch\n",
    "np.random.seed(1)\n",
    "cv_grid = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "param_grid_knn = {'n_neighbors': list(range(1,50,2))}\n",
    "method_knn = KNeighborsRegressor()\n",
    "method_tune_knn = GridSearchCV(\n",
    "        method_knn, \n",
    "        param_grid_knn, \n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv = cv_grid, \n",
    "        n_jobs =1, \n",
    "        verbose = 0\n",
    ")\n",
    "\n",
    "\n",
    "# Model evaluation\n",
    "t1 = datetime.now()\n",
    "cv_evaluation = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "scores_knn = -cross_val_score(\n",
    "        method_tune_knn, \n",
    "        X, \n",
    "        y, \n",
    "        scoring='neg_mean_squared_error', \n",
    "        cv = cv_evaluation,\n",
    "        verbose=0\n",
    ")\n",
    "tf = datetime.now() - t1\n",
    "print(tf)\n",
    "print(scores_knn)\n",
    "print(scores_knn.mean(), \"+-\", scores_knn.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a mean squared error of 0.0521, despite in one fold in the cross validation it was 0.0439. We will try to decrease it in the next steps.\n",
    "\n",
    "It took more than 25s in our computer to perform the Grid Search with cross-validation for hyper-parameter tuning and model evaluation. We could decrease this time if we increase the number of cores, since Grid Search is easy to parallelize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The best parameter for this model is: \n",
      "{'n_neighbors': 5}\n"
     ]
    }
   ],
   "source": [
    "knn_model = method_tune_knn.fit(X, y)\n",
    "print('\\nThe best parameter for this model is: ')\n",
    "print(knn_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have fitted the model and the optimum number of neighbors is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2. Decision tree. Random-search with 3-fold crossvalidation for hyper-parameter tuning to tune parameters max_depth, min_samples_split, and criterion. Model evaluation also with 4-fold crossvalidation. \n",
    "\n",
    "To use the same folds we use the the KFold objects set up in the previous chunk.\n",
    "\n",
    "In addition, we also set the seed for reproducibility purporses. \n",
    "\n",
    "The parameter grid is selected between 2 and 100 for both parameters, as a design choice. The reason is to speed up the hyper-parameter tuning.\n",
    "\n",
    "The budget is set to 40. Again, it is a nice balance, high enough to find good parameters with a more or less exhaustive Random Search, but not to high so the process takes forever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:07.267564\n",
      "[0.03325202 0.03926548 0.03841255 0.03095004]\n",
      "0.03547002037942877 +- 0.003478997154999783\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------- Random Search ----------------------------------\n",
    "\n",
    "# Hyper-parameter tuning --> Random Search\n",
    "param_grid_tree = {'max_depth': list(range(2,100,1)), \n",
    "                   'min_samples_split': list(range(2,100,1)),\n",
    "                   'criterion': ('mse', 'friedman_mse')\n",
    "}\n",
    "budget = 40\n",
    "\n",
    "np.random.seed(1)\n",
    "#cv_grid = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "method_tree = tree.DecisionTreeRegressor()\n",
    "#method_tree = tree.DecisionTreeRegressor(criterion=\"entropy\")\n",
    "method_tune_tree = RandomizedSearchCV(\n",
    "        method_tree, \n",
    "        param_grid_tree, \n",
    "        scoring='neg_mean_squared_error', \n",
    "        cv=cv_grid, \n",
    "        n_jobs=1, \n",
    "        verbose=0, \n",
    "        n_iter=budget\n",
    ")\n",
    "\n",
    "# Model evaluation\n",
    "#cv_evaluation = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "t1 = datetime.now()\n",
    "scores_tree = -cross_val_score(\n",
    "        method_tune_tree, \n",
    "        X, \n",
    "        y, \n",
    "        scoring='neg_mean_squared_error', \n",
    "        cv = cv_evaluation\n",
    ")\n",
    "tf = datetime.now() - t1\n",
    "print(tf)\n",
    "print(scores_tree)\n",
    "print(scores_tree.mean(), \"+-\", scores_tree.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error has decreased to 0.0355 (smaller than with KNN). It seems like a Decision Tree is more suitable for this problem than KNN.\n",
    "\n",
    "In addition, the time it took to perform the hyper-parameter optimization was smaller, despite we cannot compare properly with step 3.1 because it is a different algorithm and different hyper-parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The best parameter configuration for this model is: \n",
      "{'min_samples_split': 22, 'max_depth': 28, 'criterion': 'friedman_mse'}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "tree_model = method_tune_tree.fit(X, y)\n",
    "print('\\nThe best parameter configuration for this model is: ')\n",
    "print(tree_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting a final model, we show the selected parameters: __friedman mse__ as criterion, __22__ of minimum number of samples required to split an internal node and __28__ as maximum depth of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bayes Search. Decision Tree\n",
    "\n",
    "We will use the package scikit-optimize. The conditions are the same as in 3.2 (same budget, same parameter grid and same splits for the inner and outer loop).\n",
    "\n",
    "Also, we use a defined seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:39.778933\n",
      "[0.03220015 0.03857478 0.03826416 0.03083077]\n",
      "0.034967465168435 +- 0.0034875241068768707\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------- Bayes Search ----------------------------------\n",
    "\n",
    "# Hyper-parameter tuning --> Bayes Search\n",
    "np.random.seed(1)\n",
    "param_grid_bayes = {'max_depth': (2,100), \n",
    "                   'min_samples_split': (2,100),\n",
    "                   'criterion': ('mse', 'friedman_mse')                \n",
    "}\n",
    "method_tune_bayes = BayesSearchCV(\n",
    "        method_tree, \n",
    "        param_grid_bayes, \n",
    "        scoring='neg_mean_squared_error', \n",
    "        cv=cv_grid, \n",
    "        n_jobs=1, \n",
    "        verbose=0, \n",
    "        n_iter=budget\n",
    ")\n",
    "\n",
    "# Model evaluation\n",
    "t1 = datetime.now()\n",
    "scores_bayes = -cross_val_score(\n",
    "        method_tune_bayes, \n",
    "        X, \n",
    "        y, \n",
    "        scoring='neg_mean_squared_error', \n",
    "        cv = cv_evaluation\n",
    ")\n",
    "tf = datetime.now() - t1\n",
    "print(tf)\n",
    "print(scores_bayes)\n",
    "print(scores_bayes.mean(), \"+-\", scores_bayes.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we obtained a RMSE of 0.0350, slightly smaller than in the Random Search scenario. However, the performance difference is not significative, since the standard deviation is bigger than the difference. \n",
    "\n",
    "The time with Bayesian Optimization was much higher than with Random Search (2:30 minutes vs 7s in our computer). Moreover, if we increase the number of cores Bayesian Optimization will not run much faster, because it is a sequential algorithm. The only hope is that the way the scikit Decision Tree is coded allows parallelization every time a tree is computed. On the other hand, algorithms like Grid Search are easy to parallelize because they are not sequential.\n",
    "\n",
    "The warnings that we see means that the objective function has already been evaluated for the hyperparameter combination being tested at a given iteration. \n",
    "\n",
    "If we take a look at the verbose output, we observe how the iterations with smallest RMSE were in the middle. After 30 iterations the algorithm was not able to continue improving the performance. This means we could have selected a smaller budget and save time and power."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4. Determine what is your best method 3.1, 3.2, and 3.3. Use your best method to compute predictions for the competition.\n",
    "\n",
    "The cross-validation scores have already been printed, but let's show them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree with Bayes Search performance: \n",
      "0.034967465168435 +- 0.0034875241068768707\n",
      "\n",
      "Decision Tree with Random Search performance: \n",
      "0.03547002037942877 +- 0.003478997154999783\n",
      "\n",
      "KNN with Grid Search performance: \n",
      "0.05208385962541403 +- 0.007536535431943325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The best parameters for this model according to the Bayes Search are: \n",
      "{'criterion': 'friedman_mse', 'max_depth': 68, 'min_samples_split': 63}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------- Final model ----------------------------------\n",
    "np.random.seed(0)\n",
    "print('\\nDecision Tree with Bayes Search performance: ')\n",
    "print(scores_bayes.mean(), \"+-\", scores_bayes.std())\n",
    "\n",
    "print('\\nDecision Tree with Random Search performance: ')\n",
    "print(scores_tree.mean(), \"+-\", scores_tree.std())\n",
    "\n",
    "print('\\nKNN with Grid Search performance: ')\n",
    "print(scores_knn.mean(), \"+-\", scores_knn.std())\n",
    "\n",
    "# Final model fit: Decision Tree with Bayes Search\n",
    "final_model = method_tune_bayes.fit(X, y)\n",
    "print('\\nThe best parameters for this model according to the Bayes Search are: ')\n",
    "print(final_model.best_params_)\n",
    "\n",
    "# Kaggle predictions (need to obtain actual sale price computing the exponential \n",
    "# of the predicted variable)\n",
    "y_result = final_model.predict(X_comp)\n",
    "\n",
    "SalePrice = np.exp(y_result)\n",
    "output = pandas.DataFrame(SalePrice, columns = ['SalePrice'])\n",
    "output['Id'] = list(range(1461, 2920))\n",
    "output.to_csv('submission.csv', sep = ',', index = False, columns = ['Id', 'SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how the best model was Decision Tree. The best hyper-parameter configuration was chosen with Bayesian Optimization and it was: __friedman_mse__ as criterion, __68__ as maximum depth and __63__ as the minimum number of samples to split.\n",
    "\n",
    "We have to transform the predictions (exponentiate them) to submit it to the Kaggle competition because our response variable here was transformed in the pre-processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
